{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRWOI1nxutyx"
   },
   "source": [
    "# Overview\n",
    "This codelab will demonstrate how to build a LSTM model for MNIST recognition using keras & how to convert the model to TensorFlow Lite.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXzpJuM7zujk"
   },
   "outputs": [],
   "source": [
    "#!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOE_xIJuvMOU"
   },
   "source": [
    "### Prerequisites\n",
    "We're going to override the environment variable `TF_ENABLE_CONTROL_FLOW_V2` since for TensorFlow Lite control flows.\n",
    "\n",
    "It needs tensorflow version == 1.14.0 to be able to support the LSTM and Desne layers in tf.lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vsatpathy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TF_ENABLE_CONTROL_FLOW_V2 -----> This needs to be overriden and enabled.\n",
    "import os\n",
    "os.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Bidirectional, BatchNormalization, SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "numpy.random.seed(7)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Embedding is a requirement for the weight matrix as an initializer to the Embedding layer in the model.\n",
    "One can add any text file of pre-trained embedding such as Glove.\n",
    "\n",
    "This returns 2 values:\n",
    "\n",
    "    Word vocabulary\n",
    "    Embedding matrix corresponding to every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in DataSet: 400000\n"
     ]
    }
   ],
   "source": [
    "# GLOVE--EMBEDDING\n",
    "def read_data(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        word_vocab = set() # not using list to avoid duplicate entry\n",
    "        word2vector = {}\n",
    "        for line in f:\n",
    "            line_ = line.strip() #Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word2vector[words_Vec[0]] = numpy.array(words_Vec[1:],dtype=float)\n",
    "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
    "    return word_vocab,word2vector\n",
    "\n",
    "word_vocab,w2v = read_data('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process\n",
    "\n",
    "The pre-processing can vary user to user.\n",
    "\n",
    "    1. Conversion into lower text.\n",
    "    2. Removal of stop words.\n",
    "    3. Removal of single characters.\n",
    "    4. Removal of white spaces.\n",
    "\n",
    "These are the few examples to the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(text):\n",
    "    dummy=[]\n",
    "    for word in text:\n",
    "        dummy.append(str(word))\n",
    "    final=' '.join(dummy)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "Keep the stopwords while training for intent classification.\n",
    "\n",
    "In case of auto-correction stopwords need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text=str(text)\n",
    "    text=text.split(\" \")\n",
    "    text=helper(text)\n",
    "    text = str(text.lower())\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    #text = re.sub(r'[^a-zA-Z ]+', '', text)\n",
    "    # remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    #tokenize the texts using nltk\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    #######   STOPWORDS.   #######\n",
    "    #text = [word for word in text if word not in stop_words]\n",
    "    #Lemmatize the words\n",
    "    word_net_lemmatizer = WordNetLemmatizer()\n",
    "    text = [word_net_lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This process involves:\n",
    "\n",
    "    1. Reading of data from excel\n",
    "    2. Encoding the labels\n",
    "    3. Creating one unanimous DataFrame\n",
    "    4. Segregating into x,y variables for passing into the model\n",
    "    5. Tokenizing the input sequences\n",
    "    6. Padding the sequences for constant input length to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log_medication', 'content _search_details ', 'content _search_pricing', 'content _search_routine', 'content _search_safety', 'content _search_support', 'user_search_instances', 'user_search_quantity', 'user_search_last_instance', 'native_search']\n"
     ]
    }
   ],
   "source": [
    "xls=pd.ExcelFile('ML Data set.xlsx')\n",
    "df=pd.read_excel(xls,'Intent Training Set')\n",
    "\n",
    "labels=[]\n",
    "for col in df.columns:\n",
    "    labels.append(col)\n",
    "print(labels)\n",
    "master_values={}\n",
    "dummy=0\n",
    "cor_word={}\n",
    "for lab in labels:\n",
    "    master_values[lab]=df[lab].values\n",
    "    cor_word[dummy]=lab\n",
    "    dummy+=1\n",
    "\n",
    "flag=0\n",
    "concat_values=[]\n",
    "corres_labels=[]\n",
    "for key,values in master_values.items():\n",
    "    concat_values.extend(master_values[key])\n",
    "    for j in range(len(values)):\n",
    "        corres_labels.append(flag)\n",
    "    flag+=1\n",
    "    \n",
    "final_data=pd.DataFrame({'text':concat_values,'feature':corres_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.text = final_data.text.apply(preprocess)\n",
    "\n",
    "X = final_data.text\n",
    "y = final_data.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(final_data.text)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = numpy.zeros((num_words, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of embedding matrix\n",
    "\n",
    "We check if the existing word exists in the pre-trained vocab.\n",
    "\n",
    "    if True:\n",
    "        Add it to the embedding matrix for the corresponding word.\n",
    "    else:\n",
    "        pass it as dummy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_matrix = numpy.zeros(shape = (25,))\n",
    "\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        embedding_matrix[i] = dummy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3Ku1Lx9vvfX"
   },
   "source": [
    "## Step 1 Build the MNIST LSTM model.\n",
    "\n",
    "Note we will be using **`tf.lite.experimental.nn.TFLiteLSTMCell`** & **`tf.lite.experimental.nn.dynamic_rnn`**\n",
    "\n",
    "As the tflite wrapper doesn't directly support the LSTM layers of keras, hence a function is placed for the manual addition of LSTM layers with the use of tf.lit.experimental package.\n",
    "\n",
    "For more canonical lstm codelab, please see [here](https://github.com/kerasteam/keras/blob/master/examples/imdb_lstm.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiYZoDlC5SEJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 25)            4225      \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 64)                56064     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 60,939\n",
      "Trainable params: 60,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def buildLstmLayer(inputs, num_layers, num_units):\n",
    "  \"\"\"Build the lstm layer.\n",
    "\n",
    "  Args:\n",
    "    inputs: The input data.\n",
    "    num_layers: How many LSTM layers do we want.\n",
    "    num_units: The unmber of hidden units in the LSTM cell.\n",
    "    \n",
    "  \"\"\"\n",
    "  lstm_cells = []\n",
    "  for i in range(num_layers):\n",
    "    lstm_cells.append(\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, forget_bias=0, name='rnn{}'.format(i)))\n",
    "  lstm_layers = tf.keras.layers.StackedRNNCells(lstm_cells)\n",
    "  # Assume the input is sized as [batch, time, input_size], then we're going\n",
    "  # to transpose to be time-majored.\n",
    "  transposed_inputs = tf.transpose(\n",
    "      inputs, perm=[1, 0, 2])\n",
    "  outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\n",
    "      lstm_layers,\n",
    "      transposed_inputs,\n",
    "      dtype='float32',\n",
    "      time_major=True)\n",
    "  unstacked_outputs = tf.unstack(outputs, axis=0)\n",
    "  return unstacked_outputs[-1]\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=(X.shape[1],), name='input'),\n",
    "  tf.keras.layers.Embedding(num_words , 25,weights=[embedding_matrix],trainable=True,input_length=X.shape[1]),\n",
    "  tf.keras.layers.Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : 64}),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(len(labels), activation=tf.nn.softmax, name='output')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ff6X9gg_wk7K"
   },
   "source": [
    "## Step 2: Train & Evaluate the model.\n",
    "\n",
    "The data is then split into training data and testing data.\n",
    "\n",
    "Things to lookout for are the hyper parameters that need to be amended basis on the dataset size and the variance in the same. Currently used hyper parameters are:\n",
    "\n",
    "    1. Epochs\n",
    "    2. Batch size\n",
    "    \n",
    "Other hyper parameters that can come into play:\n",
    "\n",
    "    1. Learning rate\n",
    "    2. Decay ratio\n",
    "    3. Neurons per LSTM\n",
    "    \n",
    "The model training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23W41fiRPOmh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 86 samples\n",
      "Epoch 1/200\n",
      "430/430 [==============================] - 1s 3ms/sample - loss: 2.3047 - acc: 0.0884 - val_loss: 2.3018 - val_acc: 0.1163\n",
      "Epoch 2/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 2.2941 - acc: 0.1186 - val_loss: 2.1020 - val_acc: 0.2791\n",
      "Epoch 3/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 2.0125 - acc: 0.1860 - val_loss: 1.8665 - val_acc: 0.2791\n",
      "Epoch 4/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.9044 - acc: 0.1814 - val_loss: 1.8467 - val_acc: 0.2209\n",
      "Epoch 5/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.8345 - acc: 0.2163 - val_loss: 1.8021 - val_acc: 0.1977\n",
      "Epoch 6/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.8120 - acc: 0.2047 - val_loss: 1.7565 - val_acc: 0.2907\n",
      "Epoch 7/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7741 - acc: 0.2442 - val_loss: 1.7340 - val_acc: 0.2907\n",
      "Epoch 8/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.8070 - acc: 0.2116 - val_loss: 1.7214 - val_acc: 0.2674\n",
      "Epoch 9/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7957 - acc: 0.2488 - val_loss: 1.7300 - val_acc: 0.2558\n",
      "Epoch 10/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7586 - acc: 0.2163 - val_loss: 1.7168 - val_acc: 0.3023\n",
      "Epoch 11/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7521 - acc: 0.2465 - val_loss: 1.7191 - val_acc: 0.3256\n",
      "Epoch 12/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7510 - acc: 0.2581 - val_loss: 1.7243 - val_acc: 0.2674\n",
      "Epoch 13/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7433 - acc: 0.2605 - val_loss: 1.7103 - val_acc: 0.2674\n",
      "Epoch 14/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7441 - acc: 0.2395 - val_loss: 1.7076 - val_acc: 0.2907\n",
      "Epoch 15/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7435 - acc: 0.2465 - val_loss: 1.7168 - val_acc: 0.2907\n",
      "Epoch 16/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7428 - acc: 0.2535 - val_loss: 1.7082 - val_acc: 0.2791\n",
      "Epoch 17/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7410 - acc: 0.2558 - val_loss: 1.7102 - val_acc: 0.2558\n",
      "Epoch 18/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7290 - acc: 0.2651 - val_loss: 1.6994 - val_acc: 0.2907\n",
      "Epoch 19/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7931 - acc: 0.2581 - val_loss: 1.6876 - val_acc: 0.3605\n",
      "Epoch 20/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7130 - acc: 0.2721 - val_loss: 1.6650 - val_acc: 0.2907\n",
      "Epoch 21/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7606 - acc: 0.2884 - val_loss: 1.6901 - val_acc: 0.3488\n",
      "Epoch 22/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7508 - acc: 0.2581 - val_loss: 1.7078 - val_acc: 0.2791\n",
      "Epoch 23/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.7074 - acc: 0.2860 - val_loss: 1.6547 - val_acc: 0.3372\n",
      "Epoch 24/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.6664 - acc: 0.3070 - val_loss: 1.6149 - val_acc: 0.3953\n",
      "Epoch 25/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.6762 - acc: 0.3140 - val_loss: 1.6104 - val_acc: 0.3605\n",
      "Epoch 26/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.6404 - acc: 0.3302 - val_loss: 1.5863 - val_acc: 0.3605\n",
      "Epoch 27/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.6047 - acc: 0.3349 - val_loss: 1.5550 - val_acc: 0.3605\n",
      "Epoch 28/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5714 - acc: 0.3535 - val_loss: 1.5544 - val_acc: 0.3605\n",
      "Epoch 29/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5666 - acc: 0.3140 - val_loss: 1.5374 - val_acc: 0.3837\n",
      "Epoch 30/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5557 - acc: 0.3558 - val_loss: 1.5282 - val_acc: 0.4070\n",
      "Epoch 31/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5483 - acc: 0.3512 - val_loss: 1.5210 - val_acc: 0.3953\n",
      "Epoch 32/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5406 - acc: 0.3488 - val_loss: 1.5286 - val_acc: 0.3953\n",
      "Epoch 33/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5303 - acc: 0.3535 - val_loss: 1.4822 - val_acc: 0.4535\n",
      "Epoch 34/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5258 - acc: 0.3605 - val_loss: 1.5229 - val_acc: 0.3721\n",
      "Epoch 35/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5409 - acc: 0.3558 - val_loss: 1.4664 - val_acc: 0.4302\n",
      "Epoch 36/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4999 - acc: 0.3628 - val_loss: 1.4504 - val_acc: 0.4535\n",
      "Epoch 37/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4964 - acc: 0.3767 - val_loss: 1.4746 - val_acc: 0.3953\n",
      "Epoch 38/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4844 - acc: 0.3721 - val_loss: 1.4364 - val_acc: 0.4535\n",
      "Epoch 39/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4694 - acc: 0.3814 - val_loss: 1.4173 - val_acc: 0.4419\n",
      "Epoch 40/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4609 - acc: 0.3651 - val_loss: 1.4260 - val_acc: 0.4070\n",
      "Epoch 41/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4368 - acc: 0.3860 - val_loss: 1.4282 - val_acc: 0.3953\n",
      "Epoch 42/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4289 - acc: 0.3698 - val_loss: 1.4272 - val_acc: 0.3837\n",
      "Epoch 43/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4292 - acc: 0.3860 - val_loss: 1.3941 - val_acc: 0.4302\n",
      "Epoch 44/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.4395 - acc: 0.3953 - val_loss: 1.3914 - val_acc: 0.4419\n",
      "Epoch 45/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5541 - acc: 0.3907 - val_loss: 1.5160 - val_acc: 0.4186\n",
      "Epoch 46/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.6691 - acc: 0.3372 - val_loss: 1.5916 - val_acc: 0.4070\n",
      "Epoch 47/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.6030 - acc: 0.3372 - val_loss: 1.5098 - val_acc: 0.3721\n",
      "Epoch 48/200\n",
      "430/430 [==============================] - 1s 1ms/sample - loss: 1.5768 - acc: 0.3767 - val_loss: 1.5208 - val_acc: 0.3953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ad89f60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test,y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "#Addition of early stop\n",
    "early_stop=keras.callbacks.callbacks.EarlyStopping(monitor='val_acc',min_delta=0.002, patience=15)\n",
    "model.fit(X, y, epochs=200,batch_size=16,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtPJGiIQw0nM"
   },
   "source": [
    "## Step 3: Convert the Keras model to TensorFlow Lite model.\n",
    "\n",
    "Note here: we convert to TensorFlow Lite model and export it to the pre-defined path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tbuu_8PFz-x_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/util.py:202: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 27 variables.\n",
      "INFO:tensorflow:Converted 27 variables to const ops.\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/util.py:204: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
      "Model converted successfully!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.keras.backend.get_session()\n",
    "input_tensor = sess.graph.get_tensor_by_name('input:0')\n",
    "output_tensor = sess.graph.get_tensor_by_name('output/Softmax:0')\n",
    "converter = tf.lite.TFLiteConverter.from_session(\n",
    "    sess, [input_tensor], [output_tensor])\n",
    "tflite = converter.convert()\n",
    "open(\"tf_models/intent_class_update_1.tflite\",\"wb\").write(tflite)\n",
    "print('Model converted successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rHrZkIuxxar"
   },
   "source": [
    "## Step 4: Check the converted TensorFlow Lite model.\n",
    "\n",
    "We're just going to load the TensorFlow Lite model and use the TensorFlow Lite python interpreter to verify the results.\n",
    "\n",
    "The steps involved constitute of the following:\n",
    "\n",
    "    1. Giving the input text for testing\n",
    "    2. Passing it in sets of 2 words to the auto_correct function\n",
    "    3. Replicating the pre-processing methodology as used before\n",
    "    4. Loading the tflite model in ints interpreter.\n",
    "    5. Passing into the model for prediction\n",
    "    6. Restitching the corrected texts for output\n",
    "    \n",
    "The algorithm can be amended to the users liking. But the steps involved remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correct(text,tf_lite_model):\n",
    "    data = preprocess(text)\n",
    "    #print(\"data: ----->\",data)\n",
    "    tokenized = tokenizer.texts_to_sequences([data])\n",
    "    #print(\"tokenized: ----->\", tokenized)\n",
    "    padded = pad_sequences(tokenized, maxlen=max_length, padding='post')\n",
    "    ip = np.array(padded,dtype=\"float32\")\n",
    "    #print(\"ip: ----->\",ip)\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_content=tf_lite_model)\n",
    "    \n",
    "    try:\n",
    "        interpreter.allocate_tensors()\n",
    "    except ValueError:\n",
    "        assert False\n",
    "    \n",
    "    dummy=[[]]\n",
    "    if tokenized!=dummy:\n",
    "        input_index = (interpreter.get_input_details()[0]['index'])\n",
    "        print(input_index,ip)\n",
    "        interpreter.set_tensor(input_index,ip)\n",
    "        interpreter.invoke()\n",
    "        output_index = (interpreter.get_output_details()[0]['index'])\n",
    "        result = interpreter.get_tensor(output_index)\n",
    "        \n",
    "        # Reset all variables so it will not pollute other inferences.\n",
    "        interpreter.reset_all_variables()\n",
    "        return result\n",
    "    else:\n",
    "        return dummy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_word(shortlisted):\n",
    "    dummy=[]\n",
    "    for word in shortlisted:\n",
    "        if word not in dummy:\n",
    "            dummy.append(word)\n",
    "    final_text=(\" \").join(dummy)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_autocorrect = 'i want to log m gallery'\n",
    "print(\"input: -----> \",sentence_to_autocorrect)\n",
    "words = sentence_to_autocorrect.split(' ')\n",
    "shortlisted=[]\n",
    "\n",
    "for i in range(len(words)-1):\n",
    "    sub_text = words[i],words[i+1]\n",
    "    results = auto_correct(str(sub_text),tflite)\n",
    "    if len(results)>0:\n",
    "        shortlisted.append(cor_word[np.argmax(results)])\n",
    "    else:\n",
    "        shortlisted.extend(sub_text)\n",
    "    \n",
    "corrected=get_correct_word(shortlisted)\n",
    "print(\"output: -----> \",corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing for intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip: -----> [[120.  20.  45. 161.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      "[[2.2154201e-02 1.4697796e-01 1.1103599e-02 8.6035654e-03 1.3480875e-02\n",
      "  6.9347718e-03 1.0445220e-04 9.6677151e-03 5.0666803e-03 7.7590621e-01]]\n",
      "Intent:  native_search\n"
     ]
    }
   ],
   "source": [
    "input_text=\"screen is too bright\"\n",
    "data = preprocess(input_text)\n",
    "#print(\"data: ----->\",data)\n",
    "tokenized = tokenizer.texts_to_sequences([data])\n",
    "#print(\"tokenized: ----->\", tokenized)\n",
    "padded = pad_sequences(tokenized, maxlen=max_length, padding='post')\n",
    "ip = np.array(padded,dtype=\"float32\")\n",
    "print(\"ip: ----->\",ip)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite)\n",
    "\n",
    "try:\n",
    "    interpreter.allocate_tensors()\n",
    "except ValueError:\n",
    "    assert False\n",
    "    \n",
    "input_index = (interpreter.get_input_details()[0]['index'])\n",
    "interpreter.set_tensor(input_index,ip)\n",
    "interpreter.invoke()\n",
    "output_index = (interpreter.get_output_details()[0]['index'])\n",
    "result = interpreter.get_tensor(output_index)\n",
    "print(result)\n",
    "print(\"Intent: \",labels[np.argmax(result[0])])\n",
    "# Reset all variables so it will not pollute other inferences.\n",
    "interpreter.reset_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting\n",
    "The vocabulary of the new words are to be saved in a .json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('intent_class_embeddings_1.json', 'w') as f: \n",
    "    json.dump(tokenizer.word_index,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlowLite_LSTM_Keras_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
