{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install all dependencies\n",
    "\n",
    "All dependencies inclusive of the layers needed for model construction and language pre-processing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vsatpathy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vsatpathy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/vsatpathy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Embedding is a requirement for the weight matrix as an initializer to the Embedding layer in the model.\n",
    "One can add any text file of pre-trained embedding such as Glove.\n",
    "\n",
    "This returns 2 values:\n",
    "\n",
    "    Word vocabulary\n",
    "    Embedding matrix corresponding to every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in DataSet: 400000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GLOVE--EMBEDDING\n",
    "def read_data(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        word_vocab = set() # not using list to avoid duplicate entry\n",
    "        word2vector = {}\n",
    "        for line in f:\n",
    "            line_ = line.strip() #Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word2vector[words_Vec[0]] = numpy.array(words_Vec[1:],dtype=float)\n",
    "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
    "    return word_vocab,word2vector\n",
    "\n",
    "word_vocab,w2v = read_data('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process\n",
    "\n",
    "The pre-processing can vary user to user.\n",
    "\n",
    "    1. Conversion into lower text.\n",
    "    2. Removal of stop words.\n",
    "    3. Removal of single characters.\n",
    "    4. Removal of white spaces.\n",
    "\n",
    "These are the few examples to the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(text):\n",
    "    dummy=[]\n",
    "    for word in text:\n",
    "        dummy.append(str(word))\n",
    "    final=' '.join(dummy)\n",
    "    return final\n",
    "## While preprocessing total word count in corpus is stored\n",
    "word_count = []\n",
    "def preprocess(text):\n",
    "    text=str(text)\n",
    "    text=text.split(\" \")\n",
    "    text=helper(text)\n",
    "    text = str(text.lower())\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z ]+', '', text)\n",
    "    # remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    #tokenize the texts using nltk\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    #Lemmatize the words\n",
    "    #word_net_lemmatizer = WordNetLemmatizer()\n",
    "    #text = [word_net_lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This process involves:\n",
    "\n",
    "    1. Reading of data from excel\n",
    "    2. Encoding the labels\n",
    "    3. Creating one unanimous DataFrame\n",
    "    4. Segregating into x,y variables for passing into the model\n",
    "    5. Tokenizing the input sequences\n",
    "    6. Padding the sequences for constant input length to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Show Bill', 'Show Amount', 'Pay ', 'Payment mode', 'Confusion', 'Minimum due', 'Recurring payments', 'Add credit card', 'Add biller', 'Corporate']\n"
     ]
    }
   ],
   "source": [
    "#xls=pd.ExcelFile('ML Data set.xlsx')\n",
    "#df=pd.read_excel(xls,'Intent Training Set')\n",
    "#df=pd.read_excel(xls,'Auto_correct existing')\n",
    "df=pd.read_excel('USBank_Intent - RM.xlsx')\n",
    "\n",
    "labels=[]\n",
    "for col in df.columns:\n",
    "    labels.append(col)\n",
    "print(labels)\n",
    "master_values={}\n",
    "dummy=0\n",
    "cor_word={}\n",
    "for lab in labels:\n",
    "    master_values[lab]=df[lab].values\n",
    "    cor_word[dummy]=lab\n",
    "    dummy+=1\n",
    "\n",
    "flag=0\n",
    "concat_values=[]\n",
    "corres_labels=[]\n",
    "for key,values in master_values.items():\n",
    "    concat_values.extend(master_values[key])\n",
    "    for j in range(len(values)):\n",
    "        corres_labels.append(flag)\n",
    "    flag+=1\n",
    "    \n",
    "final_data=pd.DataFrame({'text':concat_values,'feature':corres_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey show bills\n",
      "show bills\n",
      "hey many bills\n",
      "hey bills pay immediately\n",
      "want pay bills\n",
      "bills mine due\n",
      "many bills\n",
      "much excel energy bill\n",
      "bill amount excel energy\n",
      "much excel energy\n",
      "much pay excel energy\n",
      "much pay excel energy\n",
      "due amount\n",
      "show details bill\n",
      "okay pay bill\n",
      "pay\n",
      "pay excel energy today\n",
      "pay electricity bill\n",
      "okay pay bill today\n",
      "want pay bill\n",
      "lets pay bill\n",
      "pay using savings account\n",
      "pay credit card\n",
      "pay credit card\n",
      "pay savings account\n",
      "pay checking account\n",
      "pay using credit card\n",
      "pay using savings account\n",
      "understand\n",
      "would\n",
      "\n",
      "mean\n",
      "understand\n",
      "would\n",
      "\n",
      "pay minimum balance credit card\n",
      "pay minimum card\n",
      "minimum avoid interest card\n",
      "pay minimum due\n",
      "lets pay minimum amount possible\n",
      "pay minimum avoid interest\n",
      "pay minimum balance credit card\n",
      "set th every month\n",
      "yes set nd\n",
      "set th\n",
      "set th\n",
      "set auto pay\n",
      "set pay date th every month\n",
      "setup recurring payment\n",
      "want add credit card\n",
      "add credit card\n",
      "please add credit card\n",
      "add new credit card\n",
      "add credit card\n",
      "please add credit card\n",
      "add new credit card\n",
      "add new biller\n",
      "want add biller\n",
      "want add new biller\n",
      "add another biller\n",
      "want add biller\n",
      "want add new biller\n",
      "add another biller\n",
      "pay corporate credit card bill\n",
      "want pay corporate credit card bill\n",
      "please pay corporate card bill\n",
      "pay corporate card bill\n",
      "want pay corporate credit card bill\n",
      "please pay corporate card bill\n",
      "pay corporate card bill\n"
     ]
    }
   ],
   "source": [
    "final_data.text = final_data.text.apply(preprocess)\n",
    "X = final_data.text\n",
    "y = final_data.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(final_data.text)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = numpy.zeros((num_words, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of embedding matrix\n",
    "\n",
    "We check if the existing word exists in the pre-trained vocab.\n",
    "\n",
    "    if True:\n",
    "        Add it to the embedding matrix for the corresponding word.\n",
    "    else:\n",
    "        pass it empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call back created\n",
    "\n",
    "In case the model takes too lonn to train or there is no development in training, callbacks can be used for terminating the training process and saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCUSRACY_THRESHOLD = .98\n",
    "\n",
    "class Call_back(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('acc') > ACCUSRACY_THRESHOLD:\n",
    "            print('Reached accuracy')\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callback = Call_back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "Things to lookout for are the hyper parameters that need to be amended basis on the dataset size and the variance in the same. Currently used hyper parameters are:\n",
    "\n",
    "    1. Epochs\n",
    "    2. Batch size\n",
    "    \n",
    "Other hyper parameters that can come into play:\n",
    "\n",
    "    1. Learning rate\n",
    "    2. Decay ratio\n",
    "    3. Neurons per LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 25)            1275      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 6,981\n",
      "Trainable params: 6,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 2.3039 - acc: 0.1143\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 2.2843 - acc: 0.2429\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 2.2375 - acc: 0.6857\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 2.0992 - acc: 0.6857\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 1.7322 - acc: 0.6714\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 1.3593 - acc: 0.7571\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 1.1074 - acc: 0.7857\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.8684 - acc: 0.8429\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.7126 - acc: 0.9571\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.6060 - acc: 0.9571\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5102 - acc: 0.9714\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.4165 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.3782 - acc: 0.9857\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.3078 - acc: 0.9714\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.2677 - acc: 0.9857\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.2269 - acc: 0.9857\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1992 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1744 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1485 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1306 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1134 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1048 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0920 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0878 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0863 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0781 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0618 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0547 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0505 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0457 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0424 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0392 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0366 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0349 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0314 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0303 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0255 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0243 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0223 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "embedding_model = Sequential()\n",
    "embedding_model.add(Embedding(num_words, 25,weights=[embedding_matrix],trainable=True,input_length=X.shape[1]))\n",
    "embedding_model.add(Bidirectional(LSTM(16)))\n",
    "embedding_model.add(Dense(len(labels), activation='softmax'))\n",
    "embedding_model.compile(optimizer=optimizers.Adam(),\n",
    "              loss=losses.sparse_categorical_crossentropy,\n",
    "              metrics=['acc'])\n",
    "embedding_model.summary()\n",
    "\n",
    "#history = embedding_model.fit(X, y, epochs=100, batch_size=2,callbacks=[callback])\n",
    "history = embedding_model.fit(X, y, epochs=100, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing and Testing\n",
    "\n",
    "The steps involved constitute of the following:\n",
    "\n",
    "    1. Giving the input text for testing\n",
    "    2. Passing it in sets of 2 words to the prediction function\n",
    "    3. Replicating the pre-processing methodology as used before\n",
    "    4. Passing into the model for prediction\n",
    "    5. Restitching the corrected texts for output\n",
    "    \n",
    "The algorithm can be amended to the users liking. But the steps involved remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_word,max_length=max_length,tokenizer=tokenizer):\n",
    "    text = [preprocess(test_word)]\n",
    "    #text=test_word\n",
    "    #print('preprocess: ----->',text)\n",
    "    tokenizer = tokenizer.texts_to_sequences(text)\n",
    "    #print('tokenizer: ----->',tokenizer)\n",
    "    embeddings = pad_sequences(tokenizer, maxlen=max_length, padding='post')\n",
    "    #print('embeddings: ----->',embeddings)\n",
    "    dummy=[[]]\n",
    "    if tokenizer!=dummy:\n",
    "        prediction1 = embedding_model.predict(embeddings)\n",
    "        #print('prediction: ----->',prediction1)\n",
    "        return prediction1\n",
    "    else:\n",
    "        return dummy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch(final_text):\n",
    "    dummy=[]\n",
    "    for i in range(len(final_text)):\n",
    "        if final_text[i] not in dummy:\n",
    "            dummy.append(final_text[i])\n",
    "    final=\" \".join(dummy)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pay\n",
      "\n",
      "credit\n",
      "credit card\n",
      "Input text:  pay with my credit card\n",
      "Corrected text:  Pay  with my Confusion Payment mode\n"
     ]
    }
   ],
   "source": [
    "input_text='pay with my credit card'\n",
    "window_size=1\n",
    "updated_text=input_text.split()\n",
    "final_text=[]\n",
    "\n",
    "for i in range(len(updated_text)):\n",
    "    if (i+window_size)<len(updated_text):\n",
    "        test_word=updated_text[i]+\" \"+updated_text[i+window_size]\n",
    "        #print('test word: ----->',test_word)\n",
    "        prob=prediction(test_word,max_length)\n",
    "        if len(prob)>0:\n",
    "            #print(test_word)\n",
    "            #print('prob: ----->',prob)\n",
    "            test_word=cor_word[numpy.argmax(prob)]\n",
    "            #print(test_word)\n",
    "            final_text.extend(test_word.split(\" \"))\n",
    "        else:\n",
    "            final_text.extend(test_word.split(\" \"))\n",
    "    else:\n",
    "        break\n",
    "corrected=stitch(final_text)\n",
    "print('Input text: ',input_text)\n",
    "print('Corrected text: ',corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for intent_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pay minimum balance credit card\n",
      "Minimum due\n"
     ]
    }
   ],
   "source": [
    "input_text_1=\"Pay the minimum balance on my credit card\"\n",
    "prob=prediction(input_text_1,max_length)\n",
    "print(labels[numpy.argmax(prob[0])])\n",
    "# tokens=[[[19.0,1.0,12.0,2.0,3.0,0.0,0.0,0.0,0.0,0.0]]]\n",
    "# out=embedding_model.predict(tokens)\n",
    "# print(labels[numpy.argmax(out)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting\n",
    "\n",
    "The model needs to be saved in a .h5 format.\n",
    "\n",
    "The vocabulary of the new words are to be saved in a .json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "embedding_model.save('ios_models/intent_class_new.h5')\n",
    "with open('embeddings_json/core_vocab_intent_class_new.json','w') as vocab:\n",
    "    json.dump(tokenizer.word_index,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "embedding_model.save('ios_models/auto_correct_new.h5')\n",
    "with open('embeddings_json/core_vocab_auto_correct_new.json','w') as vocab:\n",
    "    json.dump(tokenizer.word_index,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "embedding_model.save('intent_class_QA.h5')\n",
    "with open('core_vocab_intent_class_QA.json','w') as vocab:\n",
    "    json.dump(tokenizer.word_index,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
