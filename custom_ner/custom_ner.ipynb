{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This is important!\n",
    "import os\n",
    "os.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.lite.experimental.examples.lstm.rnn import bidirectional_dynamic_rnn\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras.models import Model,Sequential, Input, load_model\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and tokenizing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amount': 0, 'bill_type': 1, 'other': 2, 'payment_day': 3, 'payment_month': 4, 'payment_type': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab=[]\n",
    "entities=[]\n",
    "\n",
    "with open('result_usbank_1.json') as json_file:\n",
    "    dataset = json.load(json_file)\n",
    "    for data in dataset:\n",
    "    \tsent=data['sentence']\n",
    "    \ttags=data['tags']\n",
    "    \tentities.extend(tags)\n",
    "    \tfor word in sent:\n",
    "    \t\tvocab.append(word)\n",
    "json_file.close()\n",
    "\n",
    "vocab=set(vocab)\n",
    "t=Tokenizer(num_words=len(vocab))\n",
    "t.fit_on_texts(vocab)\n",
    "tokens=t.word_index\n",
    "\n",
    "num_words=len(tokens)+1\n",
    "\n",
    "reverse_tokens = dict([(value, key) for key, value in tokens.items()])\n",
    "\n",
    "entities=set(entities)\n",
    "entities=sorted(entities)\n",
    "entities=list(entities)\n",
    "label_encoder=LabelEncoder()\n",
    "int_encoded=label_encoder.fit_transform(entities)\n",
    "label_cat_encoded=to_categorical(int_encoded,num_classes=len(entities))\n",
    "\n",
    "master={}\n",
    "for i in range(len(entities)):\n",
    "\t#master[entities[i]]=int_encoded[i]\n",
    "\tmaster[entities[i]]=i\n",
    "print(master)\n",
    "\n",
    "reverse_master = dict([(value, key) for key, value in master.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation(sent,tags,tokens,master):\n",
    "    dummy_x=[]\n",
    "    dummy_y=[]\n",
    "    for word,tag in zip(sent,tags):\n",
    "        dummy_x.append(tokens[word.lower()])\n",
    "        dummy_y.append(master[tag])\n",
    "    return dummy_x,dummy_y\n",
    "\n",
    "def pre_process(sent,tokens):\n",
    "    zero_tok=0.0\n",
    "    sent=sent.split(\" \")\n",
    "    dummy_in=[]\n",
    "    for word in sent:\n",
    "        if word.lower() not in tokens:\n",
    "            dummy_in.append(zero_tok)\n",
    "        else:\n",
    "            dummy_in.append(tokens[word.lower()])\n",
    "    return dummy_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length=20\n",
    "X=[]\n",
    "Y=[]\n",
    "with open('result_usbank_1.json') as json_file:\n",
    "    dataset = json.load(json_file)\n",
    "    for data in dataset:\n",
    "        sent=data['sentence']\n",
    "        tags=data['tags']\n",
    "        tokenized_x,encoded_y=preparation(sent,tags,tokens,master)\n",
    "        X.append(tokenized_x)\n",
    "        Y.append(encoded_y)\n",
    "\n",
    "X=pad_sequences(X,maxlen=max_length,padding='post',dtype=float)\n",
    "y=pad_sequences(Y,maxlen=max_length,padding='post')\n",
    "y = y.reshape(y.shape[0],y.shape[1])\n",
    "y = to_categorical(y,num_classes=len(master))\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.2)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Bi-directional LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_layer(num_layers):\n",
    "    lstm_layers=[]\n",
    "    for i in range(num_layers):\n",
    "        lstm_layers.append(tf.lite.experimental.nn.TFLiteLSTMCell(num_units=64,name='rnn{}'.format(i),forget_bias=1.0))\n",
    "    final_lstm_layer=tf.keras.layers.StackedRNNCells(lstm_layers)\n",
    "    return final_lstm_layer\n",
    "\n",
    "def build_bidirectional(inputs,num_layers,use_dynamic_rnn=True):\n",
    "    lstm_inputs=transposed_inp=tf.transpose(inputs,[0,1,2])\n",
    "    outputs,output_states=bidirectional_dynamic_rnn(build_LSTM_layer(num_layers),build_LSTM_layer(num_layers),lstm_inputs,dtype=\"float32\",time_major=True)\n",
    "    fw_lstm_output,bw_lstm_output=outputs\n",
    "    final_out=tf.concat([fw_lstm_output,bw_lstm_output],axis=2)\n",
    "    #final_out=tf.unstack(final_out,axis=0)\n",
    "    resultant_out=final_out\n",
    "    return resultant_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build uni-directional LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstmLayer(inputs, num_layers, num_units):\n",
    "  \"\"\"Build the lstm layer.\n",
    "\n",
    "  Args:\n",
    "    inputs: The input data.\n",
    "    num_layers: How many LSTM layers do we want.\n",
    "    num_units: The unmber of hidden units in the LSTM cell.\n",
    "    \n",
    "  \"\"\"\n",
    "  lstm_cells = []\n",
    "  for i in range(num_layers):\n",
    "    lstm_cells.append(\n",
    "        tf.lite.experimental.nn.TFLiteLSTMCell(\n",
    "            num_units, forget_bias=0, name='rnn{}'.format(i)))\n",
    "  lstm_layers = tf.keras.layers.StackedRNNCells(lstm_cells)\n",
    "  # Assume the input is sized as [batch, time, input_size], then we're going\n",
    "  # to transpose to be time-majored.\n",
    "  transposed_inputs = tf.transpose(\n",
    "      inputs, perm=[0,1, 2])\n",
    "  outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\n",
    "      lstm_layers,\n",
    "      transposed_inputs,\n",
    "      dtype='float32',\n",
    "      time_major=True)\n",
    "  #unstacked_outputs = tf.unstack(outputs, axis=0)\n",
    "  #return unstacked_outputs[-1]\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 20, 32)            5408      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 20, 64)            16640     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20, 6)             390       \n",
      "=================================================================\n",
      "Total params: 22,438\n",
      "Trainable params: 22,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # model_new = tf.keras.models.Sequential([\n",
    "# #   tf.keras.Input(shape=(X.shape[1],), name='input'),\n",
    "# #   tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=32,input_length=X.shape[1],trainable=True),\n",
    "# #   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True)),\n",
    "# #   tf.keras.layers.Dense(len(entities),activation=tf.nn.softmax,name='output')\n",
    "# # ])\n",
    "\n",
    "input_layer=Input(shape=(X.shape[1],))\n",
    "model=Embedding(input_dim=len(vocab)+1,output_dim=32,input_length=X.shape[1])(input_layer)\n",
    "model = Bidirectional(LSTM(units = 32,return_sequences=True, recurrent_dropout=0.2)) (model)\n",
    "output_layer= Dense(len(master), activation=\"softmax\")(model)\n",
    "model_new = Model(input_layer,output_layer)\n",
    "\n",
    "model_new.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture for bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_l = tf.keras.Input(shape=(X.shape[1],), name='input')\n",
    "emb_l = tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=32,input_length=X.shape[1])(input_l)\n",
    "lmbda_l = tf.keras.layers.Lambda(build_bidirectional, arguments={'num_layers' : 2, 'use_dynamic_rnn': True})(emb_l)\n",
    "output_l = tf.keras.layers.Dense(len(entities),activation=tf.nn.softmax,name='output')(lmbda_l) \n",
    "model_tf = tf.keras.Model(input_l,output_l)\n",
    "model_tf.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture for uni-directional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=(X.shape[1],), name='input'),\n",
    "  tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=32,input_length=X.shape[1],trainable=True),\n",
    "  tf.keras.layers.Lambda(buildLstmLayer, arguments={'num_layers' : 2, 'num_units' : 64}),\n",
    "  tf.keras.layers.Dense(len(entities),activation=tf.nn.softmax,name='output')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "360/360 [==============================] - 1s 3ms/step - loss: 1.6469 - accuracy: 0.7007\n",
      "Epoch 2/100\n",
      "360/360 [==============================] - 0s 967us/step - loss: 0.9814 - accuracy: 0.7499\n",
      "Epoch 3/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.6776 - accuracy: 0.8132\n",
      "Epoch 4/100\n",
      "360/360 [==============================] - 0s 885us/step - loss: 0.5885 - accuracy: 0.8383\n",
      "Epoch 5/100\n",
      "360/360 [==============================] - 0s 863us/step - loss: 0.5460 - accuracy: 0.8419\n",
      "Epoch 6/100\n",
      "360/360 [==============================] - 0s 890us/step - loss: 0.5020 - accuracy: 0.8456\n",
      "Epoch 7/100\n",
      "360/360 [==============================] - 0s 896us/step - loss: 0.4564 - accuracy: 0.8478\n",
      "Epoch 8/100\n",
      "360/360 [==============================] - 0s 863us/step - loss: 0.4096 - accuracy: 0.8561\n",
      "Epoch 9/100\n",
      "360/360 [==============================] - 0s 975us/step - loss: 0.3559 - accuracy: 0.8681\n",
      "Epoch 10/100\n",
      "360/360 [==============================] - 0s 888us/step - loss: 0.2923 - accuracy: 0.8850\n",
      "Epoch 11/100\n",
      "360/360 [==============================] - 0s 868us/step - loss: 0.2282 - accuracy: 0.9258\n",
      "Epoch 12/100\n",
      "360/360 [==============================] - 0s 889us/step - loss: 0.1700 - accuracy: 0.9517\n",
      "Epoch 13/100\n",
      "360/360 [==============================] - 0s 966us/step - loss: 0.1287 - accuracy: 0.9669\n",
      "Epoch 14/100\n",
      "360/360 [==============================] - 0s 906us/step - loss: 0.0968 - accuracy: 0.9826\n",
      "Epoch 15/100\n",
      "360/360 [==============================] - 0s 928us/step - loss: 0.0737 - accuracy: 0.9924\n",
      "Epoch 16/100\n",
      "360/360 [==============================] - 0s 861us/step - loss: 0.0575 - accuracy: 0.9946\n",
      "Epoch 17/100\n",
      "360/360 [==============================] - 0s 871us/step - loss: 0.0459 - accuracy: 0.9965\n",
      "Epoch 18/100\n",
      "360/360 [==============================] - 0s 880us/step - loss: 0.0383 - accuracy: 0.9968\n",
      "Epoch 19/100\n",
      "360/360 [==============================] - 0s 874us/step - loss: 0.0312 - accuracy: 0.9974\n",
      "Epoch 20/100\n",
      "360/360 [==============================] - 0s 873us/step - loss: 0.0278 - accuracy: 0.9974\n",
      "Epoch 21/100\n",
      "360/360 [==============================] - 0s 872us/step - loss: 0.0243 - accuracy: 0.9975\n",
      "Epoch 22/100\n",
      "360/360 [==============================] - 0s 867us/step - loss: 0.0212 - accuracy: 0.9975\n",
      "Epoch 23/100\n",
      "360/360 [==============================] - 0s 925us/step - loss: 0.0198 - accuracy: 0.9976\n",
      "Epoch 24/100\n",
      "360/360 [==============================] - 0s 867us/step - loss: 0.0178 - accuracy: 0.9978\n",
      "Epoch 25/100\n",
      "360/360 [==============================] - 0s 911us/step - loss: 0.0170 - accuracy: 0.9979\n",
      "Epoch 26/100\n",
      "360/360 [==============================] - 0s 919us/step - loss: 0.0159 - accuracy: 0.9975\n",
      "Epoch 27/100\n",
      "360/360 [==============================] - 0s 874us/step - loss: 0.0147 - accuracy: 0.9978\n",
      "Epoch 28/100\n",
      "360/360 [==============================] - 0s 864us/step - loss: 0.0142 - accuracy: 0.9978\n",
      "Epoch 29/100\n",
      "360/360 [==============================] - 0s 859us/step - loss: 0.0136 - accuracy: 0.9978\n",
      "Epoch 30/100\n",
      "360/360 [==============================] - 0s 869us/step - loss: 0.0128 - accuracy: 0.9979\n",
      "Epoch 31/100\n",
      "360/360 [==============================] - 0s 970us/step - loss: 0.0122 - accuracy: 0.9978\n",
      "Epoch 32/100\n",
      "360/360 [==============================] - 0s 932us/step - loss: 0.0115 - accuracy: 0.9982\n",
      "Epoch 33/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0114 - accuracy: 0.9981\n",
      "Epoch 34/100\n",
      "360/360 [==============================] - 0s 923us/step - loss: 0.0108 - accuracy: 0.9982\n",
      "Epoch 35/100\n",
      "360/360 [==============================] - 0s 950us/step - loss: 0.0107 - accuracy: 0.9985\n",
      "Epoch 36/100\n",
      "360/360 [==============================] - 0s 947us/step - loss: 0.0102 - accuracy: 0.9985\n",
      "Epoch 37/100\n",
      "360/360 [==============================] - 0s 892us/step - loss: 0.0093 - accuracy: 0.9987\n",
      "Epoch 38/100\n",
      "360/360 [==============================] - 0s 883us/step - loss: 0.0094 - accuracy: 0.9987\n",
      "Epoch 39/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 0.9989\n",
      "Epoch 40/100\n",
      "360/360 [==============================] - 0s 886us/step - loss: 0.0089 - accuracy: 0.9987\n",
      "Epoch 41/100\n",
      "360/360 [==============================] - 0s 884us/step - loss: 0.0086 - accuracy: 0.9987\n",
      "Epoch 42/100\n",
      "360/360 [==============================] - 0s 870us/step - loss: 0.0084 - accuracy: 0.9990\n",
      "Epoch 43/100\n",
      "360/360 [==============================] - 0s 879us/step - loss: 0.0083 - accuracy: 0.9987\n",
      "Epoch 44/100\n",
      "360/360 [==============================] - 0s 855us/step - loss: 0.0079 - accuracy: 0.9990\n",
      "Epoch 45/100\n",
      "360/360 [==============================] - 0s 857us/step - loss: 0.0077 - accuracy: 0.9990\n",
      "Epoch 46/100\n",
      "360/360 [==============================] - 0s 868us/step - loss: 0.0078 - accuracy: 0.9990\n",
      "Epoch 47/100\n",
      "360/360 [==============================] - 0s 933us/step - loss: 0.0074 - accuracy: 0.9990\n",
      "Epoch 48/100\n",
      "360/360 [==============================] - 0s 882us/step - loss: 0.0073 - accuracy: 0.9990\n",
      "Epoch 49/100\n",
      "360/360 [==============================] - 0s 882us/step - loss: 0.0074 - accuracy: 0.9989\n",
      "Epoch 50/100\n",
      "360/360 [==============================] - 0s 890us/step - loss: 0.0071 - accuracy: 0.9990\n",
      "Epoch 51/100\n",
      "360/360 [==============================] - 0s 903us/step - loss: 0.0069 - accuracy: 0.9989\n",
      "Epoch 52/100\n",
      "360/360 [==============================] - 0s 847us/step - loss: 0.0071 - accuracy: 0.9990\n",
      "Epoch 53/100\n",
      "360/360 [==============================] - 0s 867us/step - loss: 0.0068 - accuracy: 0.9990\n",
      "Epoch 54/100\n",
      "360/360 [==============================] - 0s 866us/step - loss: 0.0066 - accuracy: 0.9990\n",
      "Epoch 55/100\n",
      "360/360 [==============================] - 0s 927us/step - loss: 0.0068 - accuracy: 0.9990\n",
      "Epoch 56/100\n",
      "360/360 [==============================] - 0s 864us/step - loss: 0.0066 - accuracy: 0.9990\n",
      "Epoch 57/100\n",
      "360/360 [==============================] - 0s 862us/step - loss: 0.0066 - accuracy: 0.9990\n",
      "Epoch 58/100\n",
      "360/360 [==============================] - 0s 877us/step - loss: 0.0063 - accuracy: 0.9990\n",
      "Epoch 59/100\n",
      "360/360 [==============================] - 0s 871us/step - loss: 0.0064 - accuracy: 0.9990\n",
      "Epoch 60/100\n",
      "360/360 [==============================] - 0s 886us/step - loss: 0.0061 - accuracy: 0.9990\n",
      "Epoch 61/100\n",
      "360/360 [==============================] - 0s 889us/step - loss: 0.0062 - accuracy: 0.9989\n",
      "Epoch 62/100\n",
      "360/360 [==============================] - 0s 901us/step - loss: 0.0061 - accuracy: 0.9990\n",
      "Epoch 63/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0061 - accuracy: 0.9990\n",
      "Epoch 64/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0059 - accuracy: 0.9990\n",
      "Epoch 65/100\n",
      "360/360 [==============================] - 0s 905us/step - loss: 0.0060 - accuracy: 0.9990\n",
      "Epoch 66/100\n",
      "360/360 [==============================] - 0s 873us/step - loss: 0.0057 - accuracy: 0.9990\n",
      "Epoch 67/100\n",
      "360/360 [==============================] - 0s 880us/step - loss: 0.0059 - accuracy: 0.9990\n",
      "Epoch 68/100\n",
      "360/360 [==============================] - 0s 911us/step - loss: 0.0058 - accuracy: 0.9990\n",
      "Epoch 69/100\n",
      "360/360 [==============================] - 0s 897us/step - loss: 0.0057 - accuracy: 0.9990\n",
      "Epoch 70/100\n",
      "360/360 [==============================] - 0s 878us/step - loss: 0.0057 - accuracy: 0.9990\n",
      "Epoch 71/100\n",
      "360/360 [==============================] - 0s 936us/step - loss: 0.0056 - accuracy: 0.9990\n",
      "Epoch 72/100\n",
      "360/360 [==============================] - 0s 913us/step - loss: 0.0055 - accuracy: 0.9990\n",
      "Epoch 73/100\n",
      "360/360 [==============================] - 0s 870us/step - loss: 0.0055 - accuracy: 0.9989\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 888us/step - loss: 0.0055 - accuracy: 0.9990\n",
      "Epoch 75/100\n",
      "360/360 [==============================] - 0s 862us/step - loss: 0.0051 - accuracy: 0.9990\n",
      "Epoch 76/100\n",
      "360/360 [==============================] - 0s 839us/step - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 77/100\n",
      "360/360 [==============================] - 0s 865us/step - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 78/100\n",
      "360/360 [==============================] - 0s 852us/step - loss: 0.0053 - accuracy: 0.9990\n",
      "Epoch 79/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 80/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 81/100\n",
      "360/360 [==============================] - 0s 846us/step - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 82/100\n",
      "360/360 [==============================] - 0s 896us/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 83/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 84/100\n",
      "360/360 [==============================] - 0s 948us/step - loss: 0.0051 - accuracy: 0.9990\n",
      "Epoch 85/100\n",
      "360/360 [==============================] - 0s 841us/step - loss: 0.0048 - accuracy: 0.9990\n",
      "Epoch 86/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 0.9990\n",
      "Epoch 87/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 88/100\n",
      "360/360 [==============================] - 0s 1ms/step - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 89/100\n",
      "360/360 [==============================] - 0s 890us/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 90/100\n",
      "360/360 [==============================] - 0s 943us/step - loss: 0.0047 - accuracy: 0.9990\n",
      "Epoch 91/100\n",
      "360/360 [==============================] - 0s 910us/step - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 92/100\n",
      "360/360 [==============================] - 0s 879us/step - loss: 0.0046 - accuracy: 0.9990\n",
      "Epoch 93/100\n",
      "360/360 [==============================] - 0s 859us/step - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 94/100\n",
      "360/360 [==============================] - 0s 910us/step - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 95/100\n",
      "360/360 [==============================] - 0s 908us/step - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 96/100\n",
      "360/360 [==============================] - 0s 871us/step - loss: 0.0046 - accuracy: 0.9990\n",
      "Epoch 97/100\n",
      "360/360 [==============================] - 0s 893us/step - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 98/100\n",
      "360/360 [==============================] - 0s 856us/step - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 99/100\n",
      "360/360 [==============================] - 0s 853us/step - loss: 0.0044 - accuracy: 0.9990\n",
      "Epoch 100/100\n",
      "360/360 [==============================] - 0s 867us/step - loss: 0.0045 - accuracy: 0.9990\n"
     ]
    }
   ],
   "source": [
    "def batchOutput(batch, logs):\n",
    "    print(\"Finished batch: \" + str(batch))\n",
    "\n",
    "batchLogCallback = LambdaCallback(on_batch_end=batchOutput)\n",
    "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "history = model_new.fit(train_x,train_y,epochs=100,batch_size=16)\n",
    "model_new.save('custom_ner_usbank.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(tokens['credit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5. 150. 107.  76. 122.  82. 124.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n",
      "pay -----> other\n",
      "my -----> other\n",
      "barclays -----> bill_type\n",
      "bill -----> other\n",
      "using -----> other\n",
      "credit -----> payment_type\n",
      "card -----> other\n"
     ]
    }
   ],
   "source": [
    "temp='pay my barclays bill using credit card'\n",
    "length=temp.split(\" \")\n",
    "tok_sent=pre_process(temp,tokens)\n",
    "for i in range(max_length):\n",
    "    if len(tok_sent)<max_length:\n",
    "        tok_sent.append(0.0)\n",
    "tok_sent=np.expand_dims(tok_sent,axis=0)\n",
    "print(tok_sent)\n",
    "result=model_new.predict(tok_sent)\n",
    "for i in range(len(length)):\n",
    "    ind=np.argmax(result[0][i])\n",
    "    print(length[i],\"----->\",reverse_master[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.keras.backend.get_session()\n",
    "input_tensor = sess.graph.get_tensor_by_name('input:0')\n",
    "output_tensor = sess.graph.get_tensor_by_name('output/Softmax:0')\n",
    "converter = tf.lite.TFLiteConverter.from_session(\n",
    "    sess, [input_tensor], [output_tensor])\n",
    "tflite = converter.convert()\n",
    "open(\"custom_ner_sequential_updated_1.tflite\",\"wb\").write(tflite)\n",
    "print('Model converted successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with the tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"when did i take 10 mg of olumiant in last week\"\n",
    "words=input_text.split(\" \")\n",
    "text=pre_process(input_text,tokens)\n",
    "final=[]\n",
    "for i in range(max_length):\n",
    "    if i>=len(text):\n",
    "        final.append(0)\n",
    "    else:\n",
    "        final.append(text[i])\n",
    "ip = np.array(final,dtype=\"float32\")\n",
    "ip=[ip]\n",
    "ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite)\n",
    "try:\n",
    "    interpreter.allocate_tensors()\n",
    "except ValueError:\n",
    "    assert False\n",
    "    \n",
    "input_index = (interpreter.get_input_details()[0]['index'])\n",
    "interpreter.set_tensor(input_index,ip)\n",
    "interpreter.invoke()\n",
    "output_index = (interpreter.get_output_details()[0]['index'])\n",
    "result = interpreter.get_tensor(output_index)\n",
    "print(result)\n",
    "\n",
    "for i in range(len(result[0])):\n",
    "    if i>=len(words):\n",
    "        pass\n",
    "    else:\n",
    "        tag=reverse_master[np.argmax(result[0][i])]\n",
    "        print(words[i],tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_ner_usbank.json','w') as f:\n",
    "    json.dump(tokens,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
