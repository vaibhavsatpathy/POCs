{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('employee.csv')\n",
    "columns = data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(pairs,data):\n",
    "    #reading of files for local function\n",
    "#     data = pd.read_csv('employee.csv')\n",
    "    columns = data.columns\n",
    "    # splitting into x and y\n",
    "    x = []\n",
    "    y = []\n",
    "    colx = pairs[0]\n",
    "    coly = pairs[1]\n",
    "    x.append(data[colx])\n",
    "    y.append(data[coly])\n",
    "    # reading it element wise so that we can change from numpy array to list format\n",
    "    final_x = []\n",
    "    final_y = []\n",
    "    for j in range(x[0].shape[0]):\n",
    "        final_x.append(x[0][j])\n",
    "        final_y.append(y[0][j])\n",
    "        \n",
    "    plt.scatter(final_x,final_y)\n",
    "    plt.title(colx + ' vs ' + coly)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for restaurant data post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infer(input_keys, input_token_index):\n",
    "#     input_texts = []\n",
    "\n",
    "#     name_text = input_keys['name']\n",
    "#     eatType_text = input_keys['eatType']\n",
    "#     food_text = input_keys['food']\n",
    "#     priceRange_text = input_keys['priceRange']\n",
    "#     customerRating_text = input_keys['customerRating']\n",
    "#     area_text = input_keys['area']\n",
    "#     kidsFriendly_text = input_keys['familyFriendly']\n",
    "#     near_text = input_keys['near']\n",
    "\n",
    "#     name_string = 'start_name ' + name_text + ' stop_name' if name_text else 'start_name stop_name'\n",
    "#     eatType_string = 'start_eattype ' + eatType_text + ' stop_eattype' if eatType_text else 'start_eattype stop_eattype'\n",
    "#     food_string = 'start_food ' + food_text + ' stop_food' if food_text else 'start_food stop_food'\n",
    "#     priceRange_string = 'start_pricerange ' + priceRange_text + ' stop_pricerange' if priceRange_text else 'start_pricerange stop_pricerange'\n",
    "#     customerRating_string = 'start_customerrating ' + customerRating_text + ' stop_customerrating' if customerRating_text else 'start_customerrating stop_customerrating'\n",
    "#     area_string = 'start_area ' + area_text + ' stop_area' if area_text else 'start_area stop_area'\n",
    "#     kidsFriendly_string = 'start_kidsfriendly ' + kidsFriendly_text + ' stop_kidsfriendly' if kidsFriendly_text else 'start_kidsfriendly stop_kidsfriendly'\n",
    "#     near_string = 'start_near ' + near_text + ' stop_near' if near_text else 'start_near stop_near'\n",
    "\n",
    "#     input_string = ' '.join(\n",
    "#         [name_string, eatType_string, food_string, priceRange_string, customerRating_string, area_string,\n",
    "#          kidsFriendly_string, near_string])\n",
    "#     input_texts.append(input_string)\n",
    "\n",
    "#     encoder_input_data = np.zeros(\n",
    "#         (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "#         dtype='float32')\n",
    "\n",
    "#     for i, (input_text) in enumerate(input_texts):\n",
    "#         for t, wor in enumerate(input_text.split(\" \")):\n",
    "#             encoder_input_data[i, t, input_token_index[wor]] = 1.\n",
    "\n",
    "#     return encoder_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_graphs(input_keys, input_token_index):\n",
    "    input_texts = []\n",
    "    # extracting the key and values from the dictionary\n",
    "    name = str(input_keys['mean'])\n",
    "    median_text = str(int(input_keys['median']))\n",
    "    mode_text = str(input_keys['mode'])\n",
    "    colx_text = str(input_keys['colx']).lower()\n",
    "    coly_text = str(input_keys['coly']).lower()\n",
    "    # Adding start and stop words\n",
    "    name_string = 'start_name ' + name + ' stop_name' if name else 'start_name stop_name'\n",
    "    median_string = 'start_median ' + median_text + ' stop_median' if median_text else 'start_median stop_median'\n",
    "    mode_string = 'start_mode ' + mode_text + ' stop_mode' if mode_text else 'start_mode stop_mode'\n",
    "    colx_string = 'start_colx ' + colx_text + ' stop_colx' if colx_text else 'start_colx stop_colx'\n",
    "    coly_string = 'start_coly ' + coly_text + ' stop_coly' if coly_text else 'start_coly stop_coly'\n",
    "    # appending it to the input texts\n",
    "    input_string = ' '.join(\n",
    "        [name_string, median_string, mode_string, colx_string, coly_string])\n",
    "    input_texts.append(input_string)\n",
    "    print(input_texts[0])\n",
    "    # creating the encoder data to be fed to the model\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text) in enumerate(input_texts):\n",
    "        for t, wor in enumerate(input_text.split(\" \")):\n",
    "            encoder_input_data[i, t, input_token_index[wor]] = 1.\n",
    "\n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the numbers to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, enc_model, dec_model, reverse_target_token_index):\n",
    "    # print(input_seq)\n",
    "    states_value = enc_model.predict(input_seq)  # Encode the input\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))  # Empty target sequence of length 1\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.  # Start token for target and update for each new prediction\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = dec_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_wor = reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_wor\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if (sampled_wor == '\\n' or\n",
    "                len(decoded_sentence) > max_decoder_seq_length * 2):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training module for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder_input_data, decoder_input_data, decoder_target_data):\n",
    "    # hyper parmaeters\n",
    "    batch_size = 64    # Batch size for training.\n",
    "    epochs = 15     # Number of epochs to train for.\n",
    "    latent_dim = 256   # Latent dimensionality of the encoding space.\n",
    "    \n",
    "    # Build encoder\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))      # unique input tokens\n",
    "    encoder = LSTM(latent_dim, return_state=True)                 # number of neurons\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "    encoder_states = [state_h, state_c]                           # We discard the outputs\n",
    "    \n",
    "    # Build the decoder, using the context of the encoder\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))                  # unique output tokens\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.summary()\n",
    "\n",
    "    # Run training\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    # extra parameters can be added for the output metrics such as accuracy and validation losses\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "    model.save('models/master_model_graph.h5')\n",
    "\n",
    "    # Inference model architecture\n",
    "    # encoder model\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    encoder_model.summary()\n",
    "    encoder_model.save('models/encoder_model_graph.h5')\n",
    "    # decoder model\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    decoder_model.summary()\n",
    "    decoder_model.save('models/decoder_model_graph.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics calculation for graph summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(pairs):\n",
    "    data = pd.read_csv('employee.csv')\n",
    "    columns = data.columns\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    colx = pairs[0]\n",
    "    coly = pairs[1]\n",
    "    x.append(data[colx])\n",
    "    y.append(data[coly])\n",
    "    \n",
    "    final_x = []\n",
    "    final_y = []\n",
    "    for j in range(x[0].shape[0]):\n",
    "        final_x.append(x[0][j])\n",
    "        final_y.append(y[0][j])\n",
    "    # statistical value calculations for the given pair of inputs\n",
    "    mean = statistics.mean(final_x)\n",
    "    median = statistics.median(final_x)\n",
    "    mode = statistics.mode(final_x)\n",
    "    \n",
    "    return mean, median, mode, colx, coly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling out dependent functions for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_token_index, target_token_index,encoder_input_data, pairs):\n",
    "    mean, median, mode, colx, coly = summary(pairs)\n",
    "    # reversing the token and word combination\n",
    "    reverse_input_token_index = dict(\n",
    "        (i, wor) for wor, i in input_token_index.items())\n",
    "    reverse_target_token_index = dict(\n",
    "        (i, wor) for wor, i in target_token_index.items())\n",
    "    # laoding the encoder and decoder models for inferencing\n",
    "    enc_mod = load_model('models/encoder_model_graph.h5')\n",
    "    dec_mod = load_model('models/decoder_model_graph.h5')\n",
    "\n",
    "    for seq_index in range(1):\n",
    "        # for statistical data\n",
    "        input_keys_graph = {'mean': mean, 'median': median, 'mode': mode, 'colx': colx, 'coly': coly}\n",
    "        input_seq_user = infer_graphs(input_keys_graph, input_token_index)\n",
    "        decoded_sentence = decode_sequence(input_seq_user, enc_mod, dec_mod, reverse_target_token_index)\n",
    "        \n",
    "        print('Input sentence:\\n', input_keys_graph)\n",
    "        print('---------')\n",
    "        print('Decoded sentence:\\n', decoded_sentence)\n",
    "        \n",
    "#         # for custom restaurant data\n",
    "#         input_keys = {'name': 'the mill', 'eatType': 'pub', 'food': 'english', 'priceRange': 'high',\n",
    "#                       'customerRating': '5',\n",
    "#                       'area': 'city centre', 'familyFriendly': 'no', 'near': ''}\n",
    "#         input_seq_user = infer(input_keys, input_token_index)\n",
    "#         decoded_sentence = decode_sequence(input_seq_user, enc_mod, dec_mod, reverse_target_token_index)\n",
    "\n",
    "#         print('Input sentence:\\n', input_keys)\n",
    "#         print('---------')\n",
    "#         print('Decoded sentence:\\n', decoded_sentence)\n",
    "\n",
    "#         # for data out of the training data\n",
    "#         input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "#         decoded_sentence = decode_sequence(input_seq, enc_mod, dec_mod)\n",
    "\n",
    "#         print('-')\n",
    "#         print('Input sentence:\\n', input_texts[seq_index])\n",
    "#         print('Decoded sentence:\\n', decoded_sentence)\n",
    "#         print('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep for restaurant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'e2e-dataset/trainset.csv'\n",
    "# input_texts = []\n",
    "# target_texts = []\n",
    "# input_vocab = set()\n",
    "# target_vocab = set()\n",
    "\n",
    "# with open(data_path, 'r', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     training_set = list(reader)\n",
    "\n",
    "# for element in training_set[1:]:\n",
    "#     input_text = element[0]\n",
    "\n",
    "#     name_text = re.search('(?<=name\\[).+?(?=\\])', input_text)\n",
    "#     eatType_text = re.search('(?<=eatType\\[).+?(?=\\])', input_text)\n",
    "#     food_text = re.search('(?<=food\\[).+?(?=\\])', input_text)\n",
    "#     priceRange_text = re.search('(?<=priceRange\\[).+?(?=\\])', input_text)\n",
    "#     customerRating_text = re.search('(?<=customer rating\\[).+?(?=\\])', input_text)\n",
    "#     area_text = re.search('(?<=area\\[).+?(?=\\])', input_text)\n",
    "#     kidsFriendly_text = re.search('(?<=familyFriendly\\[).+?(?=\\])', input_text)\n",
    "#     near_text = re.search('(?<=near\\[).+?(?=\\])', input_text)\n",
    "\n",
    "#     name_string = 'start_name ' + name_text.group(0) + ' stop_name' if name_text else 'start_name stop_name'\n",
    "#     eatType_string = 'start_eatType ' + eatType_text.group(\n",
    "#         0) + ' stop_eatType' if eatType_text else 'start_eatType stop_eatType'\n",
    "#     food_string = 'start_food ' + food_text.group(0) + ' stop_food' if food_text else 'start_food stop_food'\n",
    "#     priceRange_string = 'start_priceRange ' + priceRange_text.group(\n",
    "#         0) + ' stop_priceRange' if priceRange_text else 'start_priceRange stop_priceRange'\n",
    "#     customerRating_string = 'start_customerRating ' + customerRating_text.group(\n",
    "#         0) + ' stop_customerRating' if customerRating_text else 'start_customerRating stop_customerRating'\n",
    "#     area_string = 'start_area ' + area_text.group(0) + ' stop_area' if area_text else 'start_area stop_area'\n",
    "#     kidsFriendly_string = 'start_kidsFriendly ' + kidsFriendly_text.group(\n",
    "#         0) + ' stop_kidsFriendly' if kidsFriendly_text else 'start_kidsFriendly stop_kidsFriendly'\n",
    "#     near_string = 'start_near ' + near_text.group(0) + ' stop_near' if near_text else 'start_near stop_near'\n",
    "\n",
    "#     input_string = ' '.join(\n",
    "#         [name_string, eatType_string, food_string, priceRange_string, customerRating_string, area_string,\n",
    "#          kidsFriendly_string, near_string])\n",
    "#     input_texts.append(input_string)\n",
    "\n",
    "#     target_text = element[1]\n",
    "#     target_text = '\\t ' + target_text + ' \\n'\n",
    "#     target_texts.append(target_text)\n",
    "#     # print(input_string)\n",
    "#     # print(target_text)\n",
    "\n",
    "# input_vocab = set(text_to_word_sequence(\" \".join(input_texts), filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~'))\n",
    "# target_vocab = set(text_to_word_sequence(\" \".join(target_texts), filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~'))\n",
    "\n",
    "# input_text_modif = []\n",
    "# for input_text in input_texts:\n",
    "#     input_text_modif.append(' '.join(text_to_word_sequence(input_text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~', lower=True)))\n",
    "\n",
    "# target_text_modif = []\n",
    "# for target_text in target_texts:\n",
    "#     target_text_modif.append(' '.join(text_to_word_sequence(target_text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~', lower=True)))\n",
    "\n",
    "# input_texts = input_text_modif\n",
    "# target_texts = target_text_modif\n",
    "\n",
    "# input_vocab = sorted(list(input_vocab))\n",
    "# target_vocab = sorted(list(target_vocab))\n",
    "# num_encoder_tokens = len(input_vocab)\n",
    "# num_decoder_tokens = len(target_vocab)\n",
    "# max_encoder_seq_length = max([len(txt.split(\" \")) for txt in input_texts])\n",
    "# max_decoder_seq_length = max([len(txt.split(\" \")) for txt in target_texts])\n",
    "# print('Number of samples:', len(input_texts))\n",
    "# print('Number of unique input tokens:', num_encoder_tokens)\n",
    "# print('Number of unique output tokens:', num_decoder_tokens)\n",
    "# print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "# print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "# input_token_index = dict(\n",
    "#     [(wor, i) for i, wor in enumerate(input_vocab)])\n",
    "# target_token_index = dict(\n",
    "#     [(wor, i) for i, wor in enumerate(target_vocab)])\n",
    "# # print(input_token_index)\n",
    "\n",
    "# encoder_input_data = np.zeros(\n",
    "#     (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "#     dtype='float32')\n",
    "# decoder_input_data = np.zeros(\n",
    "#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "#     dtype='float32')\n",
    "# decoder_target_data = np.zeros(\n",
    "#     (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "#     dtype='float32')\n",
    "\n",
    "# for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "#     for t, wor in enumerate(input_text.split(\" \")):\n",
    "#         encoder_input_data[i, t, input_token_index[wor]] = 1.\n",
    "#     for t, wor in enumerate(target_text.split(\" \")):\n",
    "#         decoder_input_data[i, t, target_token_index[wor]] = 1.\n",
    "#         if t > 0:\n",
    "#             decoder_target_data[i, t - 1, target_token_index[wor]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep for statistics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1\n",
      "Number of unique input tokens: 15\n",
      "Number of unique output tokens: 15\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 16\n"
     ]
    }
   ],
   "source": [
    "data_path = 'dummy_data_new.csv'\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_vocab = set()\n",
    "target_vocab = set()\n",
    "\n",
    "# reading the .csv file\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set = list(reader)\n",
    "\n",
    "for element in training_set[1:]:\n",
    "    input_text = element[0]\n",
    "    # keyword search through the .csv file\n",
    "    name = re.search('(?<=mean\\[).+?(?=\\])', input_text)\n",
    "    median_text = re.search('(?<=median\\[).+?(?=\\])', input_text)\n",
    "    mode_text = re.search('(?<=mode\\[).+?(?=\\])', input_text)\n",
    "    colx_text = re.search('(?<=colx\\[).+?(?=\\])', input_text)\n",
    "    coly_text = re.search('(?<=coly\\[).+?(?=\\])', input_text)\n",
    "    # adding the start and stop words\n",
    "    name_string = 'start_name ' + name.group(0) + ' stop_name' if name else 'start_name stop_name'\n",
    "    median_string = 'start_median ' + median_text.group(\n",
    "        0) + ' stop_median' if median_text else 'start_median stop_median'\n",
    "    mode_string = 'start_mode ' + mode_text.group(0) + ' stop_mode' if mode_text else 'start_mode stop_mode'\n",
    "    colx_string = 'start_colx ' + colx_text.group(\n",
    "        0) + ' stop_colx' if colx_text else 'start_colx stop_colx'\n",
    "    coly_string = 'start_coly ' + coly_text.group(\n",
    "        0) + ' stop_coly' if coly_text else 'start_coly stop_coly'\n",
    "    # adding the processed text to input texts\n",
    "    input_string = ' '.join(\n",
    "        [name_string, median_string, mode_string, colx_string, coly_string])\n",
    "    input_texts.append(input_string)\n",
    "    # creating the labels for deep learning model\n",
    "    target_text = element[1]\n",
    "    target_text = '\\t ' + target_text + ' \\n'\n",
    "    target_texts.append(target_text)\n",
    "    # print(input_string)\n",
    "    # print(target_text)\n",
    "# creating the input and output vocab as sets so only unique characters are stored\n",
    "input_vocab = set(text_to_word_sequence(\" \".join(input_texts), filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~'))\n",
    "target_vocab = set(text_to_word_sequence(\" \".join(target_texts), filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~'))\n",
    "\n",
    "input_text_modif = []\n",
    "for input_text in input_texts:\n",
    "    input_text_modif.append(' '.join(text_to_word_sequence(input_text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~', lower=True)))\n",
    "\n",
    "target_text_modif = []\n",
    "for target_text in target_texts:\n",
    "    target_text_modif.append(' '.join(text_to_word_sequence(target_text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~', lower=True)))\n",
    "\n",
    "input_texts = input_text_modif\n",
    "target_texts = target_text_modif\n",
    "\n",
    "input_vocab = sorted(list(input_vocab))\n",
    "target_vocab = sorted(list(target_vocab))\n",
    "num_encoder_tokens = len(input_vocab)\n",
    "num_decoder_tokens = len(target_vocab)\n",
    "max_encoder_seq_length = max([len(txt.split(\" \")) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt.split(\" \")) for txt in target_texts])\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "# creating the tokens for the vocabulary for pre-processing\n",
    "input_token_index = dict(\n",
    "    [(wor, i) for i, wor in enumerate(input_vocab)])\n",
    "target_token_index = dict(\n",
    "    [(wor, i) for i, wor in enumerate(target_vocab)])\n",
    "# print(input_token_index)\n",
    "# creating the input and output tokens for the encoder - decoder model\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, wor in enumerate(input_text.split(\" \")):\n",
    "        encoder_input_data[i, t, input_token_index[wor]] = 1.\n",
    "    for t, wor in enumerate(target_text.split(\" \")):\n",
    "        decoder_input_data[i, t, target_token_index[wor]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[wor]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the trainig module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 15)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 15)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 278528      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  278528      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 15)     3855        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 560,911\n",
      "Trainable params: 560,911\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/15\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 2.5488\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.4784\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.3569\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8744\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.1846\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.7259\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.7106\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.4380\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8106\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.5648\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3523\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2119\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1203\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1200\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.5713\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 15)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 256), (None, 256) 278528    \n",
      "=================================================================\n",
      "Total params: 278,528\n",
      "Trainable params: 278,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 15)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  278528      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 15)     3855        lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 282,383\n",
      "Trainable params: 282,383\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train(encoder_input_data, decoder_input_data, decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the widgets for UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN THIS CELL TWICE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdecc05265104be6bdba7628cff1c283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(options=('Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFroâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"RUN THIS CELL TWICE\")\n",
    "# creating menu with them\n",
    "variables = widgets.Dropdown(options=columns)\n",
    "variables_1 = widgets.Dropdown(options=columns)\n",
    "# button, output, function and linkage\n",
    "butt = widgets.Button(description='Column Y')\n",
    "butt_1 = widgets.Button(description='Column X')\n",
    "out = widgets.Output()\n",
    "\n",
    "pairs = []\n",
    "\n",
    "def on_butt_clicked_1(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "#         print('column_x: ' + variables_1.value)\n",
    "        pairs.append(variables_1.value)\n",
    "\n",
    "def on_butt_clicked(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print('column_x: ' + variables_1.value)\n",
    "        print('column_y: ' + variables.value)\n",
    "        pairs.append(variables.value)\n",
    "        if len(pairs)>1:\n",
    "            display(Markdown(plots(pairs, data)))\n",
    "            display(Markdown(inference(input_token_index, target_token_index, encoder_input_data, pairs)))\n",
    "            pairs.clear()\n",
    "        \n",
    "butt.on_click(on_butt_clicked)\n",
    "butt_1.on_click(on_butt_clicked_1)\n",
    "# display\n",
    "box = widgets.VBox([variables_1, butt_1, variables, butt, out])\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
